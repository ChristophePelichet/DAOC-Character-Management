#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script de d√©bogage pour scraper un personnage depuis Herald
Affiche en d√©tail toutes les donn√©es extraites de la page
"""

import sys
from pathlib import Path

# Ajouter le dossier parent au path pour importer les modules
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from Functions.eden_scraper import EdenScraper
from Functions.cookie_manager import CookieManager
from bs4 import BeautifulSoup
import time
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def debug_scrape_character(character_url, output_file=None):
    """
    Scrape un personnage et affiche toutes les donn√©es extraites
    
    Args:
        character_url: URL du personnage sur Herald
        output_file: Fichier de sortie optionnel (sinon sauvegard√© dans Logs/)
    """
    # Create the File of sortie
    if not output_file:
        debug_dir = project_root / "Logs"
        debug_dir.mkdir(exist_ok=True)
        output_file = debug_dir / "debug_scrape_output.txt"
    
    # Ouvrir the File for √©criture
    with open(output_file, 'w', encoding='utf-8') as f:
        def log(message):
            """Affiche et √©crit dans le fichier"""
            print(message)
            f.write(message + "\n")
        
        log("=" * 80)
        log(f"D√âBOGAGE DU SCRAPING")
        log(f"URL: {character_url}")
        log("=" * 80)
        
        try:
            # Initialiser le cookie manager
            cookie_manager = CookieManager()
            
            if not cookie_manager.cookie_exists():
                log("‚ùå ERREUR: Aucun cookie trouv√©")
                return
            
            log("\n‚úÖ Cookies trouv√©s")
            
            # Initialiser le scraper
            scraper = EdenScraper(cookie_manager)
            
            if not scraper.initialize_driver(headless=False):
                log("‚ùå ERREUR: Impossible d'initialiser le navigateur")
                return
            
            log("‚úÖ Navigateur initialis√©")
            
            if not scraper.load_cookies():
                scraper.close()
                log("‚ùå ERREUR: Impossible de charger les cookies")
                return
            
            
            log("‚úÖ Cookies charg√©s dans le navigateur")
            
            # Naviguer vers l'URL
            log(f"\nüåê Navigation vers: {character_url}")
            scraper.driver.get(character_url)
            
            # Attendre que la page se charge
            log("‚è≥ Attente du chargement de la page (5 secondes)...")
            time.sleep(5)
            
            # Retrieve HTML
            page_source = scraper.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            log("\n" + "=" * 80)
            log("CONTENU DE LA PAGE")
            log("=" * 80)
            
            # Afficher le titre
            title = soup.title.string if soup.title else "Pas de titre"
            log(f"\nüìÑ Titre de la page: {title}")
            
            # Afficher the en-t√™tes
            log("\nüìã En-t√™tes H1:")
            h1_tags = soup.find_all('h1')
            if h1_tags:
                for i, h1 in enumerate(h1_tags, 1):
                    log(f"  {i}. {h1.get_text(strip=True)}")
            else:
                log("  Aucun H1 trouv√©")
            
            log("\nüìã En-t√™tes H2:")
            h2_tags = soup.find_all('h2')
            if h2_tags:
                for i, h2 in enumerate(h2_tags, 1):
                    log(f"  {i}. {h2.get_text(strip=True)}")
            else:
                log("  Aucun H2 trouv√©")
            
            log("\nüìã En-t√™tes H3:")
            h3_tags = soup.find_all('h3')
            if h3_tags:
                for i, h3 in enumerate(h3_tags, 1):
                    log(f"  {i}. {h3.get_text(strip=True)}")
            else:
                log("  Aucun H3 trouv√©")
            
            # Afficher tous les tableaux
            log("\n" + "=" * 80)
            log("TABLEAUX TROUV√âS")
            log("=" * 80)
            
            tables = soup.find_all('table')
            log(f"\nüìä Nombre de tableaux: {len(tables)}")
            
            for table_idx, table in enumerate(tables, 1):
                log(f"\n{'‚îÄ' * 80}")
                log(f"TABLEAU {table_idx}")
                log(f"{'‚îÄ' * 80}")
                
                rows = table.find_all('tr')
                log(f"Nombre de lignes: {len(rows)}\n")
                
                # Collecter toutes the Data of the tableau
                table_data = []
                max_cols = 0
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        row_data = []
                        for cell in cells:
                            cell_text = cell.get_text(strip=True)
                            tag_type = cell.name
                            row_data.append((tag_type, cell_text))
                        table_data.append(row_data)
                        max_cols = max(max_cols, len(row_data))
                
                # Calculer les largeurs de colonnes
                col_widths = [0] * max_cols
                for row_data in table_data:
                    for i, (tag, text) in enumerate(row_data):
                        col_widths[i] = max(col_widths[i], len(text) + 4)  # +4 pour [tag]
                
                # Afficher the tableau format√©
                for row_idx, row_data in enumerate(table_data, 1):
                    # Ligne of s√©paration
                    if row_idx == 1:
                        sep = "‚îå" + "‚î¨".join("‚îÄ" * w for w in col_widths) + "‚îê"
                    else:
                        sep = "‚îú" + "‚îº".join("‚îÄ" * w for w in col_widths) + "‚î§"
                    log(sep)
                    
                    # Contenu de la ligne
                    cells_formatted = []
                    for i in range(max_cols):
                        if i < len(row_data):
                            tag, text = row_data[i]
                            tag_str = f"[{tag}]"
                            cell_str = f"{tag_str} {text}"
                            cells_formatted.append(cell_str.ljust(col_widths[i]))
                        else:
                            cells_formatted.append(" " * col_widths[i])
                    
                    log("‚îÇ" + "‚îÇ".join(cells_formatted) + "‚îÇ")
                
                # Ligne de fin
                if table_data:
                    sep = "‚îî" + "‚î¥".join("‚îÄ" * w for w in col_widths) + "‚îò"
                    log(sep)
            
            # Extraction des Data with the logique actuelle
            log("\n" + "=" * 80)
            log("DONN√âES EXTRAITES (avec la logique actuelle)")
            log("=" * 80)
            
            character_data = {
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'url': character_url
            }
            
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True).lower()
                        value = cells[1].get_text(strip=True)
                        
                        log(f"\nüîç Analyse: '{key}' = '{value}'")
                        
                        # Mapper les champs
                        if 'name' in key or 'nom' in key:
                            character_data['name'] = value
                            character_data['clean_name'] = value.split()[0]
                            log(f"  ‚úÖ Trouv√©: name = {value}")
                        elif 'level' in key or 'niveau' in key:
                            character_data['level'] = value
                            log(f"  ‚úÖ Trouv√©: level = {value}")
                        elif 'class' in key or 'classe' in key:
                            character_data['class'] = value
                            log(f"  ‚úÖ Trouv√©: class = {value}")
                        elif 'race' in key:
                            character_data['race'] = value
                            log(f"  ‚úÖ Trouv√©: race = {value}")
                        elif 'realm' in key or 'royaume' in key:
                            character_data['realm'] = value
                            log(f"  ‚úÖ Trouv√©: realm = {value}")
                        elif 'guild' in key or 'guilde' in key:
                            character_data['guild'] = value
                            log(f"  ‚úÖ Trouv√©: guild = {value}")
                        elif 'realm point' in key or 'points de royaume' in key:
                            character_data['realm_points'] = value
                            log(f"  ‚úÖ Trouv√©: realm_points = {value}")
                        elif 'realm rank' in key or 'rang de royaume' in key:
                            character_data['realm_rank'] = value
                            log(f"  ‚úÖ Trouv√©: realm_rank = {value}")
                        elif 'realm level' in key or 'niveau de royaume' in key:
                            character_data['realm_level'] = value
                            log(f"  ‚úÖ Trouv√©: realm_level = {value}")
                        elif 'server' in key or 'serveur' in key:
                            character_data['server'] = value
                            log(f"  ‚úÖ Trouv√©: server = {value}")
            
            log("\n" + "=" * 80)
            log("R√âSULTAT FINAL")
            log("=" * 80)
            
            log("\nüì¶ Donn√©es extraites:")
            for key, value in character_data.items():
                log(f"  {key}: {value}")
            
            # Check if on a the Data minimales
            if 'name' in character_data or 'class' in character_data:
                log("\n‚úÖ SUCC√àS: Donn√©es minimales extraites")
            else:
                log("\n‚ùå √âCHEC: Impossible d'extraire les donn√©es minimales (name ou class)")
            
            # Sauvegarder le HTML brut pour inspection
            debug_dir = project_root / "Logs"
            debug_dir.mkdir(exist_ok=True)
            html_file = debug_dir / "debug_herald_page.html"
            
            with open(html_file, 'w', encoding='utf-8') as hf:
                hf.write(page_source)
            
            log(f"\nüíæ HTML brut sauvegard√© dans: {html_file}")
            
            scraper.close()
            
        except Exception as e:
            log(f"\n‚ùå EXCEPTION: {e}")
            import traceback
            log(traceback.format_exc())


if __name__ == "__main__":
    # URL par d√©faut - can be modifi√©e
    default_url = "https://eden-daoc.net/herald?n=player&k=Ewo"
    
    if len(sys.argv) > 1:
        character_url = sys.argv[1]
    else:
        character_url = default_url
        print(f"Utilisation de l'URL par d√©faut: {character_url}")
        print("Pour utiliser une autre URL: python debug_scrape_character.py <URL>\n")
    
    debug_scrape_character(character_url)
    
    # Afficher o√π the File a √©t√© sauvegard√©
    output_file = project_root / "Logs" / "debug_scrape_output.txt"
    print("\n" + "=" * 80)
    print("D√âBOGAGE TERMIN√â")
    print(f"üìÑ R√©sultats sauvegard√©s dans: {output_file}")
    print("=" * 80)