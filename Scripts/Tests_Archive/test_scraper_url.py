#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Test script pour scraper une URL Eden avec le EdenScraper
"""

import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from Functions.eden_scraper import EdenScraper
from Functions.cookie_manager import CookieManager
from Functions.config_manager import config

def test_scrape_url(url):
    """Test scraping d'une URL spÃ©cifique"""
    print("=" * 80)
    print(f"TEST SCRAPING URL: {url}")
    print("=" * 80)
    
    # Initialize cookie manager
    print("\n1. Initialisation du Cookie Manager...")
    cookie_manager = CookieManager()
    
    # Initialize scraper
    print("\n2. Initialisation du Scraper Eden...")
    scraper = EdenScraper(cookie_manager)
    
    # Initialize driver (headless mode)
    print("\n3. Initialisation du driver Selenium (mode VISIBLE)...")
    if not scraper.initialize_driver(headless=False):
        print("âŒ ERREUR: Impossible d'initialiser le driver")
        return
    
    print("âœ… Driver initialisÃ©")
    
    # Load cookies
    print("\n4. Chargement des cookies...")
    if not scraper.load_cookies():
        print("âŒ ERREUR: Impossible de charger les cookies")
        scraper.close()
        return
    
    print("âœ… Cookies chargÃ©s")
    
    # Navigate to URL
    print(f"\n5. Navigation vers {url}...")
    try:
        scraper.driver.get(url)
        
        # Wait for page load
        import time
        time.sleep(3)
        
        # Get page source
        html_content = scraper.driver.page_source
        current_url = scraper.driver.current_url
        
        print(f"\nâœ… Page chargÃ©e")
        print(f"URL actuelle: {current_url}")
        print(f"Taille HTML: {len(html_content)} caractÃ¨res")
        
        # Parse with BeautifulSoup
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract interesting data
        print("\n" + "=" * 80)
        print("CONTENU DE LA PAGE:")
        print("=" * 80)
        
        # Title
        title = soup.find('title')
        if title:
            print(f"\nğŸ“„ TITRE: {title.get_text(strip=True)}")
        
        # Main content
        print("\nğŸ“‹ CONTENU PRINCIPAL:\n")
        
        # Look for common elements
        main_content = soup.find('main') or soup.find('div', class_='content') or soup.body
        
        if main_content:
            # Get text content (limited to first 2000 chars)
            text_content = main_content.get_text(separator='\n', strip=True)
            print(text_content[:2000])
            
            if len(text_content) > 2000:
                print(f"\n... (contenu tronquÃ©, {len(text_content)} caractÃ¨res au total)")
        
        # Look for specific elements (tables, forms, etc.)
        print("\n" + "=" * 80)
        print("Ã‰LÃ‰MENTS STRUCTURELS:")
        print("=" * 80)
        
        tables = soup.find_all('table')
        print(f"\nğŸ“Š Tables trouvÃ©es: {len(tables)}")
        
        forms = soup.find_all('form')
        print(f"ğŸ“ Formulaires trouvÃ©s: {len(forms)}")
        
        links = soup.find_all('a')
        print(f"ğŸ”— Liens trouvÃ©s: {len(links)}")
        
        # Check for item-specific elements
        items = soup.find_all(class_=lambda x: x and 'item' in x.lower())
        print(f"ğŸ’ Ã‰lÃ©ments 'item' trouvÃ©s: {len(items)}")
        
        # Save HTML to file for inspection
        debug_file = project_root / 'Logs' / 'debug_market_page.html'
        debug_file.parent.mkdir(exist_ok=True)
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"\nğŸ’¾ HTML sauvegardÃ© dans: {debug_file}")
        
    except Exception as e:
        print(f"\nâŒ ERREUR lors du scraping: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Close driver
        print("\n6. Fermeture du driver...")
        scraper.close()
        print("âœ… Driver fermÃ©")

if __name__ == "__main__":
    url = "https://eden-daoc.net/items?m=market"
    test_scrape_url(url)
